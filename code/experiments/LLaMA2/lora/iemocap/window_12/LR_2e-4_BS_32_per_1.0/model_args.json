{
     "model_type": "decoder",
     "model_name_or_path": "../../models/llama2-7b",
     "checkpoint_dir": null,
     "output_dir": "./experiments/LLaMA2/lora/iemocap/window_12/LR_2e-4_BS_32_per_1.0",
     "data_dir": "../original_data/iemocap/window",
     "do_train": true,
     "do_eval": true,
     "warmup_ratio": 0.1,
     "warmup_steps": 98,
     "save_steps": 100000,
     "weight_decay": 0.0,
     "max_seq_length": 256,
     "max_length": 1200,
     "num_beams": 1,
     "do_sample": false,
     "top_k": null,
     "top_p": null,
     "learning_rate": 0.0002,
     "preprocess_inputs": true,
     "clip_norm": 1.0,
     "open_ended": false,
     "batch_size": 32,
     "eval_batch_size": 8,
     "gradient_accumulation_steps": 8,
     "lora": true,
     "lora_dim": 16,
     "lora_alpha": 16,
     "lora_dropout": 0.05,
     "lora_module_name": "q_proj,k_proj,v_proj,query_key_value",
     "seed": 42,
     "offload_optimizer": false,
     "deepspeed_config": "./data_utils/deepspeed_config.json",
     "zero_shot": true,
     "mode": "sft",
     "gradient_checkpointing": false
}